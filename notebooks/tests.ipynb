{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statistics\n",
    "import glob\n",
    "\n",
    "dfs = []\n",
    "for f in glob.glob(\"/tmp/data/SPI_*.csv.zip\"):\n",
    "    dfs.append(pd.read_csv(f))\n",
    "SPI = pd.concat(dfs)\n",
    "\n",
    "dfs = []\n",
    "for f in glob.glob(\"/tmp/data/AOI_*.csv.zip\"):\n",
    "    dfs.append(pd.read_csv(f))\n",
    "AOI = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1 = set(classification_1(SPI))\n",
    "results_2 = classification_2(SPI, AOI[[\"PanelID\",\"FigureID\",\"MachineID\",\"ComponentID\",\"PinNumber\",\"AOILabel\"]])\n",
    "results_3 = classification_3(SPI, AOI[[\"PanelID\",\"FigureID\",\"MachineID\",\"ComponentID\",\"PinNumber\",\"AOILabel\",\"OperatorLabel\"]])\n",
    "\n",
    "# Performance Task 1\n",
    "groundtruth_1  = {tuple( [str(f) for f in e] ) for e in AOI[[\"PanelID\",\"FigureID\",\"ComponentID\"]].values}\n",
    "precision_1    = len(results_1&groundtruth_1)/len(results_1) if len(results_1) > 0 else 0\n",
    "recall_1       = len(results_1&groundtruth_1)/len(groundtruth_1) if len(groundtruth_1) > 0 else 0\n",
    "f1_1           = 2*precision_1*recall_1/(precision_1+recall_1) if precision_1+recall_1 > 0 else 0\n",
    "\n",
    "# Performance Task 2\n",
    "results_dict_2 = { (str(p), str(f), str(c)):l for p, f, c, l in results_2}\n",
    "validationdata_2 = []\n",
    "for t in AOI.drop_duplicates(subset=[\"PanelID\",\"FigureID\",\"ComponentID\"], keep=\"first\").itertuples():\n",
    "    predicted = results_dict_2.get(( str(t.PanelID), str(t.FigureID), str(t.ComponentID)), \"-\" )\n",
    "    validationdata_2.append((t.PanelID, t.FigureID, t.ComponentID, t.OperatorLabel, predicted))\n",
    "validationdata_2 = pd.DataFrame(validationdata_2, columns = [\"PanelID\",\"FigureID\",\"ComponentID\", \"Real\", \"Predicted\"]) \n",
    "f1_2 = classification_report(validationdata_2[\"Real\"], validationdata_2[\"Predicted\"],output_dict=True)[\"Bad\"][\"f1-score\"]\n",
    "\n",
    "# Performance Task 3\n",
    "results_dict_3 = { (str(p), str(f), str(c)):l for p, f, c, l in results_3}\n",
    "validationdata_3 = []\n",
    "for t in AOI[AOI[\"RepairLabel\"].isin({\"FalseScrap\",\"NotPossibleToRepair\"})]\\\n",
    "        .drop_duplicates(subset=[\"PanelID\",\"FigureID\",\"ComponentID\"], keep=\"first\").itertuples():\n",
    "    predicted = results_dict_3.get(( str(t.PanelID), str(t.FigureID), str(t.ComponentID)), \"-\" )\n",
    "    validationdata_3.append((t.PanelID, t.FigureID, t.ComponentID, t.RepairLabel, predicted))\n",
    "validationdata_3 = pd.DataFrame(validationdata_3, columns = [\"PanelID\",\"FigureID\",\"ComponentID\", \"Real\", \"Predicted\"]) \n",
    "cr = classification_report(validationdata_3[\"Real\"], validationdata_3[\"Predicted\"],output_dict=True)\n",
    "f1_3 = (cr[\"FalseScrap\"][\"f1-score\"] + cr[\"NotPossibleToRepair\"][\"f1-score\"])/2\n",
    "\n",
    "print(\"F1 Score Task 1:\", f1_1)\n",
    "print(\"F1 Score Task 2:\", f1_2)\n",
    "print(\"F1 Score Task 3:\", f1_3)\n",
    "print(\"Final Score:\", statistics.mean([f1_1, f1_2, f1_3]))\n",
    "\n",
    "mean = statistics.mean([f1_1, f1_2, f1_3])\n",
    "\n",
    "print(f\"{f1_1},{f1_2},{f1_3},{mean}\", file = open(\"results-tests.txt\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
